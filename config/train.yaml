##########################################
# Training Parameters
##########################################
seed: 42        # Common seed across all source of randomness
batch_size: 
  smallscale: 128 # 128
  largescale: 64  # batch size is 64 because of the memory constraint but increase the epochs to 120, then, all data can be seen enough time to the model.
epochs:
  smallscale: 70 # 70
  largescale: 120 # 120
num_workers: 5      # Dataloader number of workers
training_debug_1: False     # Check featrue space. Only with LeNet++
training_debug_2: False     # Check weights and gradient

##########################################
# Data Parameters
##########################################
data:
  smallscale:
    root: /local/scratch/hkim # EMNIST path
    split_ratio: 0.8
    label_filter: [-1]        # [-1] for no filtering. ex. [1,2] only use sample '1' and '2'
  largescale:
    root: /local/scratch/datasets/ImageNet/ILSVRC2012 # ILSVRC2012 path
    protocol: ../../data/LargeScale # ImageNet Protocol root path
  train_neg_size: -1 # -1 for using all. SmallScale : < 42000 (42240)  LargeScale_2 : < 60000 (60689)
  # 1000 5000 10000 20000 30000 -1
##########################################
# Archtecture Parameters
##########################################
arch:
  feat_dim: -1    # -1 for using default value.

##########################################
# Loss Parameters
##########################################
loss:
  eos:
    unkn_weight: 1
  osovr:
    sigma:        # LeNet_plus_plus : 6     LeNet : 8       ResNet_50 : 8
      LeNet_plus_plus: 8    # 6 (04.10.2024)
      LeNet: 8            # 6 (04.10.2024)
      ResNet_50: 8
    mode:
      # C: 'global'   # Small : 'global'     Large : 'global'
      # F: 2       # Small : 2    Large : 2            
      M: 0.4      # Small : 0     Large : 0.4
  ovr:
    mode:
      # C: 'batch'   # Small : 'batch'     Large : 'batch'
      # F: 2        # Small : 0.2    Large : 2            
      M: 0.6      # Small : 0.2     Large : 0.6

##########################################
# Optimizer Parameters
##########################################
opt:    # Adam optimizer
  lr: 1.e-3  # 1.e-3 / 1.e-4 (P3 OSOvR). Initial learning rate
  lr2: 1.e-4  # P3 OSOvR, P2 M 0.4 OvR
  decay: 0    # 0. Number of epochs to wait for each learning rate reduction. 0 means no decay
  gamma: 1    # 1. Factor to reduce the learning rate

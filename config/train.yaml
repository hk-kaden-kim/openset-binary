##########################################
# Training Parameters
##########################################
seed: 42        # Common seed across all source of randomness
batch_size: 
  smallscale: 128 # 128
  largescale: 64  # batch size is 64 because of the memory constraint but increase the epochs to 120, then, all data can be seen enough time to the model.
epochs:
  smallscale: 70 # 70
  largescale: 120 # 120
num_workers: 5      # Dataloader number of workers

##########################################
# Data Parameters
##########################################
data:
  smallscale:
    root: /local/scratch/hkim # EMNIST path
    split_ratio: 0.8
  largescale:
    root: /local/scratch/datasets/ImageNet/ILSVRC2012 # ILSVRC2012 path
    protocol: ../../data/LargeScale # ImageNet Protocol root path
  train_neg_size: 0 # -1 for using all. SmallScale : < 42000 (42240)  LargeScale_2 : < 60000 (60689)

##########################################
# Data Parameters
##########################################

##########################################
# Loss Parameters
##########################################
loss:
  eos:
    unkn_weight: 1
  osovr:
    sigma: 5        # LeNet_plus_plus : 6     LeNet : 8       ResNet_50 : 5 6 8 11 15
    mode:
      # C: 'batch'   # 'global' or 'batch'
      # F: 3        # 0.2 0.6 1 2 3
      # M: 0.8      # 0 or 0.2 or 0.4 or 0.6 or 0.8
  ovr:
    mode:
      # C: 'batch'   # 'global' or 'batch'
      # F: 3        # 0.2 0.6 1 2 3
      # M: 0.8      # 0 or 0.2 or 0.4 or 0.6 or 0.8

##########################################
# Optimizer Parameters
##########################################
opt:
  solver: adam  # Two options: {adam, sgd}
  lr: 1.e-3   # 1.e-3. Initial learning rate
  decay: 0    # 0. Number of epochs to wait for each learning rate reduction. 0 means no decay
  gamma: 1    # 1. Factor to reduce the learning rate

##########################################
# Training Parameters
##########################################
seed: 42        # Common seed across all source of randomness
batch_size: 
  smallscale: 128 # 128
  largescale: 16  # batch size is 64 because of the memory constraint.
epochs:
  smallscale: 70 # 70
  largescale: 120 # 120
num_workers: 5      # Dataloader number of workers
training_debug_1: False     # Check featrue space. Only with LeNet++
training_debug_2: False     # Check weights and gradient

##########################################
# Data Parameters
##########################################
data:
  smallscale:
    root: /local/scratch/hkim # EMNIST path
    split_ratio: 0.8
  largescale:
    root: /local/scratch/datasets/ImageNet/ILSVRC2012 # ILSVRC2012 path
    protocol: ../../data/LargeScale # ImageNet Protocol root path
  train_neg_size: -1 # -1 for using all. SmallScale : < 42000 (42240)  LargeScale_2 : < 60000 (60689)

##########################################
# Archtecture Parameters
##########################################
arch:
  feat_dim: -1    # -1 for using default value.

##########################################
# Loss Parameters
##########################################
loss:
  eos:
    unkn_weight: 1
  ovr:
    mode: # (Scale) : (Neg 0)/(Neg All)
      # C: 'batch'   # Small : 'global' / 'batch'     Large : 'global' / 'batch'
      # F: 1          # Small : 2 / 1           Large : 1 / 3            
      # M: 0.2       # Small : 0.4 / 0.2         Large : 0.2 / 0.2

##########################################
# Optimizer Parameters
##########################################
opt:    # Adam optimizer
  lr: 1.e-3  
  decay: 0    # 0. Number of epochs to wait for each learning rate reduction. 0 means no decay
  gamma: 1    # 1. Factor to reduce the learning rate
